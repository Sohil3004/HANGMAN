{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486d3507",
   "metadata": {},
   "source": [
    "# Part 2: Reinforcement Learning Agent for Hangman\n",
    "\n",
    "This notebook implements the RL component that uses the HMM from Part 1 to play Hangman optimally.\n",
    "\n",
    "## Approach\n",
    "\n",
    "### RL Design:\n",
    "- **State**: Combination of:\n",
    "  - Masked word pattern\n",
    "  - Guessed letters\n",
    "  - Remaining lives\n",
    "  - HMM probability distribution\n",
    "  - Game progress (blanks remaining)\n",
    "\n",
    "- **Actions**: Guess any unguessed letter (a-z)\n",
    "\n",
    "- **Reward Function**:\n",
    "  - +100: Win the game\n",
    "  - -100: Lose the game\n",
    "  - -10: Wrong guess (lose a life)\n",
    "  - +5: Correct guess (reveal letters)\n",
    "  - -5: Repeated guess (inefficiency penalty)\n",
    "\n",
    "### Implementation:\n",
    "We'll use **Deep Q-Network (DQN)** with:\n",
    "1. Experience replay buffer\n",
    "2. Target network for stability\n",
    "3. ε-greedy exploration strategy\n",
    "4. State representation using HMM probabilities + game features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad52015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque, Counter, defaultdict\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Set\n",
    "import sys\n",
    "\n",
    "# For neural network\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "    USE_DQN = True\n",
    "    print(f\"PyTorch available: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    USE_DQN = False\n",
    "    print(\"PyTorch not available, will use Q-Table approach\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf5930",
   "metadata": {},
   "source": [
    "## 1. Load Data and HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7febf24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus\n",
    "with open('data/corpus.txt', 'r') as f:\n",
    "    corpus = [word.strip().lower() for word in f.readlines()]\n",
    "\n",
    "# Load test set\n",
    "with open('data/test.txt', 'r') as f:\n",
    "    test_words = [word.strip().lower() for word in f.readlines()]\n",
    "\n",
    "print(f\"Corpus: {len(corpus)} words\")\n",
    "print(f\"Test set: {len(test_words)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained HMM model\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import the HMM class from part 1\n",
    "exec(open('part1_hmm_model.ipynb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e704b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Load HMM from pickle if already trained\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class HangmanHMM:\n",
    "    \"\"\"Simplified HMM class for loading\"\"\"\n",
    "    def __init__(self):\n",
    "        self.corpus_by_length = defaultdict(list)\n",
    "        self.position_freq = {}\n",
    "        self.letter_freq = Counter()\n",
    "        self.bigram_freq = defaultdict(Counter)\n",
    "        self.trigram_freq = defaultdict(Counter)\n",
    "        self.alphabet = set('abcdefghijklmnopqrstuvwxyz')\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        print(\"Training HMM...\")\n",
    "        for word in corpus:\n",
    "            word = word.lower()\n",
    "            self.corpus_by_length[len(word)].append(word)\n",
    "        \n",
    "        for length, words in self.corpus_by_length.items():\n",
    "            self.position_freq[length] = defaultdict(Counter)\n",
    "            for word in words:\n",
    "                for pos, char in enumerate(word):\n",
    "                    self.position_freq[length][pos][char] += 1\n",
    "                    self.letter_freq[char] += 1\n",
    "                for i in range(len(word) - 1):\n",
    "                    self.bigram_freq[word[i]][word[i+1]] += 1\n",
    "                for i in range(len(word) - 2):\n",
    "                    self.trigram_freq[word[i:i+2]][word[i+2]] += 1\n",
    "        \n",
    "        self._normalize_frequencies()\n",
    "        print(\"HMM training complete!\")\n",
    "    \n",
    "    def _normalize_frequencies(self):\n",
    "        for length in self.position_freq:\n",
    "            for pos in self.position_freq[length]:\n",
    "                total = sum(self.position_freq[length][pos].values())\n",
    "                for char in self.position_freq[length][pos]:\n",
    "                    self.position_freq[length][pos][char] /= total\n",
    "        \n",
    "        for first_char in self.bigram_freq:\n",
    "            total = sum(self.bigram_freq[first_char].values())\n",
    "            for second_char in self.bigram_freq[first_char]:\n",
    "                self.bigram_freq[first_char][second_char] /= total\n",
    "        \n",
    "        for bigram in self.trigram_freq:\n",
    "            total = sum(self.trigram_freq[bigram].values())\n",
    "            for char in self.trigram_freq[bigram]:\n",
    "                self.trigram_freq[bigram][char] /= total\n",
    "    \n",
    "    def predict_letter_probabilities(self, masked_word: str, guessed_letters: set) -> Dict[str, float]:\n",
    "        \"\"\"Main prediction method\"\"\"\n",
    "        remaining_letters = self.alphabet - guessed_letters\n",
    "        if not remaining_letters:\n",
    "            return {}\n",
    "        \n",
    "        # Get frequency-based probabilities\n",
    "        total = sum(self.letter_freq[letter] for letter in remaining_letters)\n",
    "        if total == 0:\n",
    "            return {letter: 1.0/len(remaining_letters) for letter in remaining_letters}\n",
    "        \n",
    "        return {letter: self.letter_freq[letter] / total for letter in remaining_letters}\n",
    "    \n",
    "    def get_best_guess(self, masked_word: str, guessed_letters: set) -> str:\n",
    "        probs = self.predict_letter_probabilities(masked_word, guessed_letters)\n",
    "        if not probs:\n",
    "            remaining = self.alphabet - guessed_letters\n",
    "            return max(remaining, key=lambda x: self.letter_freq.get(x, 0))\n",
    "        return max(probs.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "# Train HMM\n",
    "hmm = HangmanHMM()\n",
    "hmm.train(corpus)\n",
    "print(f\"Most common letters: {hmm.letter_freq.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bf676",
   "metadata": {},
   "source": [
    "## 2. Hangman Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f10045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanEnv:\n",
    "    \"\"\"\n",
    "    Hangman game environment for RL training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, word_list: List[str], max_wrong_guesses: int = 6):\n",
    "        self.word_list = word_list\n",
    "        self.max_wrong_guesses = max_wrong_guesses\n",
    "        self.alphabet = set('abcdefghijklmnopqrstuvwxyz')\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, word: str = None):\n",
    "        \"\"\"\n",
    "        Reset the environment to start a new game.\n",
    "        \"\"\"\n",
    "        if word is None:\n",
    "            self.word = random.choice(self.word_list).lower()\n",
    "        else:\n",
    "            self.word = word.lower()\n",
    "        \n",
    "        self.guessed_letters = set()\n",
    "        self.wrong_guesses = 0\n",
    "        self.repeated_guesses = 0\n",
    "        self.done = False\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_masked_word(self) -> str:\n",
    "        \"\"\"Get the current masked word.\"\"\"\n",
    "        return ''.join([c if c in self.guessed_letters else '_' for c in self.word])\n",
    "    \n",
    "    def _get_state(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the current state of the game.\n",
    "        \"\"\"\n",
    "        masked_word = self._get_masked_word()\n",
    "        return {\n",
    "            'masked_word': masked_word,\n",
    "            'guessed_letters': self.guessed_letters.copy(),\n",
    "            'wrong_guesses': self.wrong_guesses,\n",
    "            'lives_remaining': self.max_wrong_guesses - self.wrong_guesses,\n",
    "            'word_length': len(self.word),\n",
    "            'blanks_remaining': masked_word.count('_'),\n",
    "            'letters_revealed': len(self.word) - masked_word.count('_'),\n",
    "            'done': self.done\n",
    "        }\n",
    "    \n",
    "    def step(self, action: str) -> Tuple[Dict, float, bool]:\n",
    "        \"\"\"\n",
    "        Take an action (guess a letter) and return next state, reward, done.\n",
    "        \n",
    "        Args:\n",
    "            action: Letter to guess\n",
    "        \n",
    "        Returns:\n",
    "            state: New state\n",
    "            reward: Reward for this action\n",
    "            done: Whether the game is over\n",
    "        \"\"\"\n",
    "        action = action.lower()\n",
    "        reward = 0\n",
    "        \n",
    "        # Check for repeated guess\n",
    "        if action in self.guessed_letters:\n",
    "            self.repeated_guesses += 1\n",
    "            reward = -5  # Penalty for repeated guess\n",
    "            return self._get_state(), reward, self.done\n",
    "        \n",
    "        # Add to guessed letters\n",
    "        self.guessed_letters.add(action)\n",
    "        \n",
    "        # Check if letter is in word\n",
    "        if action in self.word:\n",
    "            # Correct guess\n",
    "            num_revealed = self.word.count(action)\n",
    "            reward = 5 * num_revealed  # Reward proportional to letters revealed\n",
    "            \n",
    "            # Check if won\n",
    "            if self._get_masked_word().count('_') == 0:\n",
    "                reward += 100  # Big bonus for winning\n",
    "                self.done = True\n",
    "        else:\n",
    "            # Wrong guess\n",
    "            self.wrong_guesses += 1\n",
    "            reward = -10  # Penalty for wrong guess\n",
    "            \n",
    "            # Check if lost\n",
    "            if self.wrong_guesses >= self.max_wrong_guesses:\n",
    "                reward -= 100  # Big penalty for losing\n",
    "                self.done = True\n",
    "        \n",
    "        return self._get_state(), reward, self.done\n",
    "    \n",
    "    def get_valid_actions(self) -> Set[str]:\n",
    "        \"\"\"Get set of valid actions (unguessed letters).\"\"\"\n",
    "        return self.alphabet - self.guessed_letters\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Print the current game state.\"\"\"\n",
    "        print(f\"Word: {self._get_masked_word()}\")\n",
    "        print(f\"Guessed: {sorted(self.guessed_letters)}\")\n",
    "        print(f\"Wrong guesses: {self.wrong_guesses}/{self.max_wrong_guesses}\")\n",
    "        print(f\"Lives: {self.max_wrong_guesses - self.wrong_guesses}\")\n",
    "\n",
    "print(\"HangmanEnv class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment\n",
    "env = HangmanEnv(['apple', 'banana', 'cherry'])\n",
    "state = env.reset('apple')\n",
    "print(\"Initial state:\")\n",
    "env.render()\n",
    "\n",
    "print(\"\\nGuessing 'e':\")\n",
    "state, reward, done = env.step('e')\n",
    "print(f\"Reward: {reward}, Done: {done}\")\n",
    "env.render()\n",
    "\n",
    "print(\"\\nGuessing 'a':\")\n",
    "state, reward, done = env.step('a')\n",
    "print(f\"Reward: {reward}, Done: {done}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e82d7d",
   "metadata": {},
   "source": [
    "## 3. State Representation\n",
    "\n",
    "We need to convert game state into a feature vector for the RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEncoder:\n",
    "    \"\"\"\n",
    "    Encodes game state into feature vector for RL agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hmm: HangmanHMM):\n",
    "        self.hmm = hmm\n",
    "        self.alphabet = list('abcdefghijklmnopqrstuvwxyz')\n",
    "        self.letter_to_idx = {letter: i for i, letter in enumerate(self.alphabet)}\n",
    "    \n",
    "    def encode(self, state: Dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode state into feature vector.\n",
    "        \n",
    "        Features:\n",
    "        - HMM probability distribution (26 features)\n",
    "        - Guessed letters one-hot (26 features)\n",
    "        - Game progress features (6 features)\n",
    "        \n",
    "        Total: 58 features\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 1. HMM probability distribution\n",
    "        hmm_probs = self.hmm.predict_letter_probabilities(\n",
    "            state['masked_word'], \n",
    "            state['guessed_letters']\n",
    "        )\n",
    "        for letter in self.alphabet:\n",
    "            features.append(hmm_probs.get(letter, 0.0))\n",
    "        \n",
    "        # 2. Guessed letters (binary)\n",
    "        for letter in self.alphabet:\n",
    "            features.append(1.0 if letter in state['guessed_letters'] else 0.0)\n",
    "        \n",
    "        # 3. Game progress features (normalized)\n",
    "        features.append(state['wrong_guesses'] / 6.0)  # Wrong guesses ratio\n",
    "        features.append(state['lives_remaining'] / 6.0)  # Lives remaining ratio\n",
    "        features.append(state['blanks_remaining'] / state['word_length'])  # Blanks ratio\n",
    "        features.append(state['letters_revealed'] / state['word_length'])  # Revealed ratio\n",
    "        features.append(state['word_length'] / 20.0)  # Word length (normalized)\n",
    "        features.append(len(state['guessed_letters']) / 26.0)  # Guess progress\n",
    "        \n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "    def get_feature_size(self) -> int:\n",
    "        \"\"\"Get the size of the feature vector.\"\"\"\n",
    "        return 26 + 26 + 6  # 58 features\n",
    "\n",
    "print(\"StateEncoder class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test state encoder\n",
    "encoder = StateEncoder(hmm)\n",
    "env = HangmanEnv(['apple'])\n",
    "state = env.reset('apple')\n",
    "features = encoder.encode(state)\n",
    "\n",
    "print(f\"Feature vector size: {len(features)}\")\n",
    "print(f\"First 10 features (HMM probs): {features[:10]}\")\n",
    "print(f\"Features 26-36 (guessed letters): {features[26:36]}\")\n",
    "print(f\"Last 6 features (game progress): {features[-6:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860d0e4",
   "metadata": {},
   "source": [
    "## 4. Deep Q-Network (DQN) Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde09b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DQN:\n",
    "    class DQN(nn.Module):\n",
    "        \"\"\"\n",
    "        Deep Q-Network for Hangman.\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, input_size: int, output_size: int):\n",
    "            super(DQN, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_size, 256)\n",
    "            self.fc2 = nn.Linear(256, 256)\n",
    "            self.fc3 = nn.Linear(256, 128)\n",
    "            self.fc4 = nn.Linear(128, output_size)\n",
    "            \n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = self.fc4(x)\n",
    "            return x\n",
    "    \n",
    "    print(\"DQN network defined successfully!\")\n",
    "else:\n",
    "    print(\"Skipping DQN (PyTorch not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience replay buffer for DQN training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience to buffer.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of experiences.\"\"\"\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"ReplayBuffer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18aaa5",
   "metadata": {},
   "source": [
    "## 5. Q-Learning Agent (Simple Approach)\n",
    "\n",
    "We'll implement a simpler Q-table approach first, then optionally use DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26518f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent that combines HMM predictions with learned Q-values.\n",
    "    Uses a hybrid approach: HMM for probabilities + Q-learning for strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hmm: HangmanHMM, alpha: float = 0.1, gamma: float = 0.95, epsilon: float = 1.0):\n",
    "        self.hmm = hmm\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        self.alphabet = list('abcdefghijklmnopqrstuvwxyz')\n",
    "        self.q_table = defaultdict(lambda: {letter: 0.0 for letter in self.alphabet})\n",
    "        \n",
    "        # Statistics\n",
    "        self.training_stats = {\n",
    "            'rewards': [],\n",
    "            'wins': [],\n",
    "            'wrong_guesses': [],\n",
    "            'epsilons': []\n",
    "        }\n",
    "    \n",
    "    def _get_state_key(self, state: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Create a hashable state key for Q-table.\n",
    "        Uses simplified state representation.\n",
    "        \"\"\"\n",
    "        masked = state['masked_word']\n",
    "        lives = state['lives_remaining']\n",
    "        blanks = state['blanks_remaining']\n",
    "        guessed = ''.join(sorted(state['guessed_letters']))\n",
    "        return f\"{masked}_{lives}_{blanks}_{guessed}\"\n",
    "    \n",
    "    def choose_action(self, state: Dict, valid_actions: Set[str], training: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Choose action using epsilon-greedy strategy combined with HMM.\n",
    "        \"\"\"\n",
    "        if not valid_actions:\n",
    "            return random.choice(self.alphabet)\n",
    "        \n",
    "        # Exploration: random action\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.choice(list(valid_actions))\n",
    "        \n",
    "        # Exploitation: combine Q-values with HMM probabilities\n",
    "        state_key = self._get_state_key(state)\n",
    "        hmm_probs = self.hmm.predict_letter_probabilities(\n",
    "            state['masked_word'], \n",
    "            state['guessed_letters']\n",
    "        )\n",
    "        \n",
    "        # Combine Q-values and HMM probabilities\n",
    "        action_scores = {}\n",
    "        for action in valid_actions:\n",
    "            q_value = self.q_table[state_key].get(action, 0.0)\n",
    "            hmm_prob = hmm_probs.get(action, 0.0)\n",
    "            \n",
    "            # Weighted combination\n",
    "            # Early in training: rely more on HMM\n",
    "            # Later: rely more on learned Q-values\n",
    "            hmm_weight = 0.7 if self.epsilon > 0.5 else 0.3\n",
    "            action_scores[action] = hmm_weight * hmm_prob + (1 - hmm_weight) * q_value\n",
    "        \n",
    "        return max(action_scores.items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    def update(self, state: Dict, action: str, reward: float, next_state: Dict, done: bool):\n",
    "        \"\"\"\n",
    "        Update Q-value using Q-learning update rule.\n",
    "        \"\"\"\n",
    "        state_key = self._get_state_key(state)\n",
    "        next_state_key = self._get_state_key(next_state)\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[state_key][action]\n",
    "        \n",
    "        # Maximum Q-value for next state\n",
    "        if done:\n",
    "            max_next_q = 0\n",
    "        else:\n",
    "            max_next_q = max(self.q_table[next_state_key].values())\n",
    "        \n",
    "        # Q-learning update\n",
    "        new_q = current_q + self.alpha * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state_key][action] = new_q\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def train(self, env: HangmanEnv, num_episodes: int, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Train the agent.\n",
    "        \"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                valid_actions = env.get_valid_actions()\n",
    "                action = self.choose_action(state, valid_actions, training=True)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                \n",
    "                self.update(state, action, reward, next_state, done)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Record statistics\n",
    "            self.training_stats['rewards'].append(total_reward)\n",
    "            self.training_stats['wins'].append(1 if total_reward > 0 else 0)\n",
    "            self.training_stats['wrong_guesses'].append(env.wrong_guesses)\n",
    "            self.training_stats['epsilons'].append(self.epsilon)\n",
    "            \n",
    "            # Decay epsilon\n",
    "            self.decay_epsilon()\n",
    "            \n",
    "            if verbose and (episode + 1) % 1000 == 0:\n",
    "                recent_wins = sum(self.training_stats['wins'][-1000:])\n",
    "                recent_reward = np.mean(self.training_stats['rewards'][-1000:])\n",
    "                print(f\"Episode {episode + 1}/{num_episodes} - \"\n",
    "                      f\"Win Rate: {recent_wins/10:.1f}%, \"\n",
    "                      f\"Avg Reward: {recent_reward:.2f}, \"\n",
    "                      f\"Epsilon: {self.epsilon:.3f}\")\n",
    "\n",
    "print(\"QLearningAgent class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae7103b",
   "metadata": {},
   "source": [
    "## 6. Train the RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d098e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agent\n",
    "train_env = HangmanEnv(corpus, max_wrong_guesses=6)\n",
    "agent = QLearningAgent(hmm, alpha=0.1, gamma=0.95, epsilon=1.0)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"This will take several minutes...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b41ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "NUM_TRAINING_EPISODES = 10000\n",
    "agent.train(train_env, num_episodes=NUM_TRAINING_EPISODES, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd1edf",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e375ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Moving average function\n",
    "def moving_average(data, window=100):\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# 1. Rewards over time\n",
    "rewards_ma = moving_average(agent.training_stats['rewards'], window=100)\n",
    "axes[0, 0].plot(rewards_ma)\n",
    "axes[0, 0].set_xlabel('Episode (100-episode MA)')\n",
    "axes[0, 0].set_ylabel('Average Reward')\n",
    "axes[0, 0].set_title('Training Rewards Over Time')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Win rate over time\n",
    "wins_ma = moving_average(agent.training_stats['wins'], window=100)\n",
    "axes[0, 1].plot(wins_ma)\n",
    "axes[0, 1].set_xlabel('Episode (100-episode MA)')\n",
    "axes[0, 1].set_ylabel('Win Rate')\n",
    "axes[0, 1].set_title('Win Rate Over Time')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Wrong guesses over time\n",
    "wrong_ma = moving_average(agent.training_stats['wrong_guesses'], window=100)\n",
    "axes[1, 0].plot(wrong_ma)\n",
    "axes[1, 0].set_xlabel('Episode (100-episode MA)')\n",
    "axes[1, 0].set_ylabel('Average Wrong Guesses')\n",
    "axes[1, 0].set_title('Wrong Guesses Over Time')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Epsilon decay\n",
    "axes[1, 1].plot(agent.training_stats['epsilons'])\n",
    "axes[1, 1].set_xlabel('Episode')\n",
    "axes[1, 1].set_ylabel('Epsilon')\n",
    "axes[1, 1].set_title('Exploration Rate (Epsilon) Decay')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal training statistics:\")\n",
    "print(f\"Final win rate (last 1000 episodes): {sum(agent.training_stats['wins'][-1000:])/10:.1f}%\")\n",
    "print(f\"Final average reward: {np.mean(agent.training_stats['rewards'][-1000:]):.2f}\")\n",
    "print(f\"Final epsilon: {agent.epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504fc79",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, word_list, max_games=2000, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate agent performance on a list of words.\n",
    "    \"\"\"\n",
    "    env = HangmanEnv(word_list, max_wrong_guesses=6)\n",
    "    \n",
    "    wins = 0\n",
    "    total_wrong_guesses = 0\n",
    "    total_repeated_guesses = 0\n",
    "    results = []\n",
    "    \n",
    "    test_words = word_list[:max_games]\n",
    "    \n",
    "    for i, word in enumerate(test_words):\n",
    "        state = env.reset(word)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            action = agent.choose_action(state, valid_actions, training=False)\n",
    "            state, reward, done = env.step(action)\n",
    "        \n",
    "        won = env.wrong_guesses < 6\n",
    "        if won:\n",
    "            wins += 1\n",
    "        \n",
    "        total_wrong_guesses += env.wrong_guesses\n",
    "        total_repeated_guesses += env.repeated_guesses\n",
    "        results.append((word, won, env.wrong_guesses, env.repeated_guesses))\n",
    "        \n",
    "        if verbose and (i + 1) % 200 == 0:\n",
    "            print(f\"Progress: {i+1}/{max_games} games completed\")\n",
    "    \n",
    "    return {\n",
    "        'wins': wins,\n",
    "        'total_games': len(test_words),\n",
    "        'success_rate': wins / len(test_words),\n",
    "        'total_wrong_guesses': total_wrong_guesses,\n",
    "        'avg_wrong_guesses': total_wrong_guesses / len(test_words),\n",
    "        'total_repeated_guesses': total_repeated_guesses,\n",
    "        'avg_repeated_guesses': total_repeated_guesses / len(test_words),\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508efad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating RL agent on test set...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "test_results = evaluate_agent(agent, test_words, max_games=2000, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RL AGENT EVALUATION RESULTS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf213c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate final score\n",
    "num_games = test_results['total_games']\n",
    "wins = test_results['wins']\n",
    "success_rate = test_results['success_rate']\n",
    "total_wrong = test_results['total_wrong_guesses']\n",
    "total_repeated = test_results['total_repeated_guesses']\n",
    "avg_wrong = test_results['avg_wrong_guesses']\n",
    "avg_repeated = test_results['avg_repeated_guesses']\n",
    "\n",
    "# Calculate score using the formula from problem statement\n",
    "final_score = (success_rate * num_games) - (total_wrong * 5) - (total_repeated * 2)\n",
    "\n",
    "print(f\"\\nGames Played: {num_games}\")\n",
    "print(f\"Games Won: {wins}\")\n",
    "print(f\"Success Rate: {success_rate:.2%}\")\n",
    "print(f\"\\nTotal Wrong Guesses: {total_wrong}\")\n",
    "print(f\"Avg Wrong Guesses per Game: {avg_wrong:.2f}\")\n",
    "print(f\"\\nTotal Repeated Guesses: {total_repeated}\")\n",
    "print(f\"Avg Repeated Guesses per Game: {avg_repeated:.2f}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL SCORE: {final_score:.2f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Compare with HMM-only baseline\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPARISON WITH HMM-ONLY BASELINE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"HMM-only score: -53207.00\")\n",
    "print(f\"RL Agent score: {final_score:.2f}\")\n",
    "print(f\"Improvement: {final_score - (-53207):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c1612",
   "metadata": {},
   "source": [
    "## 9. Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da75d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results by word length\n",
    "results_df = pd.DataFrame(\n",
    "    test_results['results'], \n",
    "    columns=['word', 'won', 'wrong_guesses', 'repeated_guesses']\n",
    ")\n",
    "results_df['word_length'] = results_df['word'].apply(len)\n",
    "\n",
    "print(\"Performance by Word Length:\")\n",
    "length_stats = results_df.groupby('word_length').agg({\n",
    "    'won': ['count', 'sum', 'mean'],\n",
    "    'wrong_guesses': 'mean',\n",
    "    'repeated_guesses': 'mean'\n",
    "})\n",
    "print(length_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0868cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Success rate by length\n",
    "length_success = results_df.groupby('word_length')['won'].mean()\n",
    "axes[0, 0].bar(length_success.index, length_success.values, color='green', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Word Length')\n",
    "axes[0, 0].set_ylabel('Success Rate')\n",
    "axes[0, 0].set_title('RL Agent: Success Rate by Word Length')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Average wrong guesses by length\n",
    "length_wrong = results_df.groupby('word_length')['wrong_guesses'].mean()\n",
    "axes[0, 1].bar(length_wrong.index, length_wrong.values, color='red', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Word Length')\n",
    "axes[0, 1].set_ylabel('Avg Wrong Guesses')\n",
    "axes[0, 1].set_title('RL Agent: Average Wrong Guesses by Word Length')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Distribution of wrong guesses\n",
    "axes[1, 0].hist(results_df['wrong_guesses'], bins=7, color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Wrong Guesses per Game')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Wrong Guesses')\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Win/Loss distribution\n",
    "win_loss_counts = results_df['won'].value_counts()\n",
    "axes[1, 1].bar(['Lost', 'Won'], \n",
    "               [win_loss_counts.get(False, 0), win_loss_counts.get(True, 0)], \n",
    "               color=['red', 'green'], alpha=0.7)\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "axes[1, 1].set_title('Win/Loss Distribution')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d9401",
   "metadata": {},
   "source": [
    "## 10. Save the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a691233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Q-table and agent configuration\n",
    "agent_data = {\n",
    "    'q_table': dict(agent.q_table),\n",
    "    'alpha': agent.alpha,\n",
    "    'gamma': agent.gamma,\n",
    "    'epsilon': agent.epsilon,\n",
    "    'training_stats': agent.training_stats,\n",
    "    'test_results': test_results\n",
    "}\n",
    "\n",
    "with open('rl_agent.pkl', 'wb') as f:\n",
    "    pickle.dump(agent_data, f)\n",
    "\n",
    "print(\"RL agent saved to rl_agent.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6b4a6",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### RL Agent Design:\n",
    "- **Algorithm**: Q-Learning with ε-greedy exploration\n",
    "- **State Representation**: \n",
    "  - Masked word pattern\n",
    "  - Guessed letters\n",
    "  - Lives remaining\n",
    "  - Game progress metrics\n",
    "- **Action Space**: 26 letters (a-z)\n",
    "- **Reward Structure**:\n",
    "  - +100 for winning\n",
    "  - -100 for losing\n",
    "  - +5 per letter revealed (correct guess)\n",
    "  - -10 per wrong guess\n",
    "  - -5 per repeated guess\n",
    "\n",
    "### Hybrid Strategy:\n",
    "The RL agent combines:\n",
    "1. **HMM Probabilities** - Statistical letter predictions\n",
    "2. **Learned Q-Values** - Experience-based strategic decisions\n",
    "3. **Adaptive Weighting** - Balances HMM and Q-values based on training progress\n",
    "\n",
    "### Key Insights:\n",
    "- Early training: Relies heavily on HMM guidance (70% weight)\n",
    "- Later training: Shifts toward learned Q-values (70% weight)\n",
    "- ε-greedy exploration ensures thorough learning\n",
    "- Simplified state representation enables efficient Q-table storage\n",
    "\n",
    "### Performance:\n",
    "The combined HMM + RL approach should significantly outperform the HMM-only baseline by learning:\n",
    "- When to trust HMM predictions\n",
    "- When to explore alternative strategies\n",
    "- Game-specific patterns and tactics\n",
    "- Optimal action selection under uncertainty"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
